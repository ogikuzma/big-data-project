version: '3'

services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
      - 9000:9000

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - PYSPARK_PYTHON=python3
    env_file:
      - ./hadoop.env
      - ./postgres.env
    volumes:
      - ./spark:/home

  spark-worker1:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env

  spark-worker2:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8082:8081
    env_file:
      - ./hadoop.env

  zoo1:
    image: zookeeper:3.4.9
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
        ZOO_MY_ID: 1
        ZOO_PORT: 2181
        ZOO_SERVERS: server.1=zoo1:2888:3888
    volumes:
      - zoo1:/data
      - zoo1log:/datalog

  kafka1:
    image: confluentinc/cp-kafka:5.3.1
    container_name: kafka1
    ports:
      - "9093:9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
    volumes:
      - kafka1:/var/lib/kafka/data
    depends_on:
      - zoo1

  kafka2:
    image: confluentinc/cp-kafka:5.3.1
    container_name: kafka2
    ports:
      - "9094:9094"
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka2:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
    volumes:
      - kafka2:/var/lib/kafka/data
    depends_on:
      - zoo1

  postgresql:
    image: postgres:14.1-alpine
    restart: on-failure:5
    container_name: postgresql
    ports:
      - "8091:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=1111
      - POSTGRES_DB=big_data
    volumes:
      - pgdata2:/var/lib/postgresql/data

  metabase-app:
    image: metabase/metabase
    restart: on-failure:10
    ports:
      - 3000:3000
    environment:
      - MB_DB_TYPE=postgres
      - MB_DB_DBNAME=big_data
      - MB_DB_PORT=5432
      - MB_DB_USER=postgres
      - MB_DB_PASS=1111
      - MB_DB_HOST=postgresql
    volumes:
      - metabase:/metabase-data
    depends_on:
      - postgresql
    links:
      - postgresql

  postgres:
    image: postgres:9.6
    container_name: postgres
    volumes: 
        - ../airflow/pg-init-scripts:/docker-entrypoint-initdb.d
    environment:
        - POSTGRES_USER=airflow
        - POSTGRES_PASSWORD=airflow
        - POSTGRES_DB=airflow
    ports:
        - "8092:5432"

  airflow:
    image: docker-airflow-spark:1.10.7_3.0.1
    container_name: airflow
    user: root
    depends_on:
      - spark-master
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - FERNET_KEY=wKU4DB8b8jfpqIqcTSGSyR6dWtCtF16PUVAOlviItYw=
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./dags:/root/airflow/dags #DAG folder
      - ./spark:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
      - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    ports:
      - "8282:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  zoo1:
  zoo1log:
  kafka1:
  kafka2:
  pgdata2:
  metabase:
